#!/bin/bash

echo "starting scal logistic short "
export LANG=C
export LC_ALL=C
R=100


L=10
R=100
#export LAMBDA=0.0001
TOPIC=$1
#NDOCS=`cat docfils | wc -l`
total_relevantes=0
#export NDUN=`wc -l < docfils`
export NDUN=23300

if [ -f synthetic_seed ]; then
    cat synthetic_seed > seed.$TOPIC
else
    echo "file not exists /tmp/seed_syntetic"
fi


rm -f new*.$TOPIC tail*.$TOPIC self*.$TOPIC gold*.$TOPIC
rm -f sub_new*.$TOPIC 
touch new00.$TOPIC
#rm rel.$TOPIC.fil
rm -f trainset

# 
# 1: Find a relevant seed document using ad-hoc search, or # construct a synthetic relevant document from the topic # description.
# 2: The initial training set consists of the seed docrecaument identified in step 1, labeled “relevant.”

echo "# 3: Draw a large uniform random sample U of size N from the document population."
    shuf -n  $NDUN  docfils | cut -d$'\t' -f2  > sample.$TOPIC


echo "# 4: Select a sub-sample size n."
    cat sample.$TOPIC | shuf -n 30 | cut -d$'\t' -f2           > sub_N.$TOPIC

echo "# 4: join 1 sub- sample."
    ./dd_script.sh  sample.201 > sample.fill

    #sort -k2 sample.$TOPIC | join -11 -21 - svm.fil  > sample.fill
    #sort -k2 sample.$TOPIC | join -11 - svm.fil |  sort -n > sample.fill

echo "# 4: join 2 sub- sample."
    #sort -k2 seed.$TOPIC | join - -12 -21  svm.fil  >> sample.fill
    ./dd_script.sh  seed.$TOPIC >> sample.fill
    cat sample.fill | sort -k1 | uniq > temp 
    mv temp sample.fill
    cp sample.fill w

    cat w | join -  -1 1 rel.201.fil | cut -d' ' -f2- | sed -e 's/^/1 /'  > sample.labelled
    cat w | join -  -1 1 rel.201.fil -v1 | cut -d' ' -f2- | sed -e 's/^/-1 /' >> sample.labelled
    
echo "# 5: Set the initial batch size B to 1."
    B=1

echo "# 6: Set R̂ to 0."
    R=0
    labelEffort=0
    truePositive=0
    i=0
    export u_size=`wc -l < w`
    echo "u_size $u_size----"
    while [ $u_size -gt 10 ]
    do
        i=$(($i+1))
        export N=$i
    
        
        
        cat seed.$TOPIC sub_new*.$TOPIC | uniq > seed    
        cat seed | sort | uniq | join - rel.$TOPIC.fil | cut -d' ' -f1 | sed -e 's/^/1 /' > x
        cat seed | sort | uniq | join - rel.$TOPIC.fil -v1 | cut -d' ' -f1 | sed -e 's/^/-1 /' >> x
    
    echo "# 7: Temporarily augment the training set by adding 100 random documents from the U , temporarily labeled “not relevant.”"
        cat w | shuf -n 100 | sort | join -v1 - rel.$TOPIC.fil | cut -d' ' -f1 | sed -e 's/^/-1 /' >> x
   

    echo "# 8: Construct a classifier from the training set."
        sort -k2 x | join -12 - sample.fill | cut -d' ' -f2-  | sort -n > trainset
    
        
    echo " run classifier"    
        ../sof*/src/sof*ml --iterations 2000000 --training_file trainset --dimensionality 1100000 --model_out svm_model
        ../sof*/src/sof*ml --test_file sample.labelled --dimensionality 1100000 --model_in svm_model --results_file pout 

        cp svm_model svm_model$N

    echo "# 10: Select the highest-scoring B  $B documents from U ."

        cat w | cut -d' ' -f1 | tr -d ' ' > docfils_temp
        python sort.py docfils_temp pout seed 1
                
        tail -$B  newtopic.sort  | cut -d' ' -f1 >  new$N.$TOPIC

    echo "11: If R̂ = 1 or B ≤ n, let b = B; otherwise let b = n."
    
    echo "valor atual do B ---- $B e do b é $b"
        if [ "$R" -eq 1 ] || [ "$B" -le 30 ]
        then
                export b=`echo $B`
        
        else
                export b=`echo 30`
        
        fi
        
            echo "valor do b $b"
            
            
    echo "# 12: Draw a random sub-sample of size b from the B docu-ments."
        shuf -n $b  new$N.$TOPIC > sub_new$N.$TOPIC

        
    echo  " 13: Review the sub-sample, labeling each as “relevant” or “not relevant.”"
        labelEffort=$(($labelEffort+$b))


    # 14: Add the labeled sub-sample to the training set.

    echo "# 15: Remove the B documents from U ."
        sort  new$N.$TOPIC | join -v2 - w > temp 
        mv temp w
        
        echo "                      novo valor para o sample `wc -l w`"
    echo "# 16: Add r·B  to R̂, where r is the number of relevant documents in the sub-sample."

        export r=`cat sub_new$N.$TOPIC | sort | uniq | join - rel.$TOPIC.fil | uniq | wc -l`
        export total=`cat sub_new$N.$TOPIC |  wc -l`
    echo "numero de documentos relevantes coletados---- $r de um total de $total"
    #cat sub_new$N.$TOPIC


        temp=$(($r*$B))
        temp1=$(($temp/$b))
        export R=$(($R+$temp1))
        
        echo "novo R $R---"
        echo "$i $R" >> valor_R
        cat newtopic.sort > U$N
    echo "# 17: Increase B by 10"

        temp=`echo "scale=5; ($B / 10)" | bc`

        int=`echo $temp |  awk '{print ($0-int($0)>0)?int($0)+1:int($0)}'`
        echo "temp ----------$temp----- $int"
        #int=${temp%.*}
        export B=$(($B+$int))

        echo "novo B ---- $B"

        export u_size=`wc -l < w`


        RELTRAINDOC=`grep -E "^1\b" trainset | wc -l`
        NOTRELTRAINDOC=`grep -E "^-1\b" trainset | wc -l`
        PREVALENCERATE=`echo "scale=4; $RELTRAINDOC / ($RELTRAINDOC + $NOTRELTRAINDOC)" | bc`
        #echo $RELTRAINDOC $NOTRELTRAINDOC $PREVALENCERATE
        truePositive=$(($RELTRAINDOC))

        echo "----------------------------------------$labelEffort  $truePositive"



    # 18: Repeat steps 7 through 16 until U is exhausted.
    # 19: Train the final classifier on all labeled documents.
            
        

done   
echo " 20: Estimate ρ̂ = 1.05 "
    export prevalence=`echo "scale=5; ($R * 1.05) / $NDUN  " | bc`
    echo "prevalence $prevalence"        
    export m=`echo "scale=5; ($prevalence * $NDUN ) * 0.95 " | bc`           
    int=${m%.*}
        
echo "# 19: Train the final classifier on all labeled documents."

    cat seed.$TOPIC sub_new*.$TOPIC synthetic_seed | uniq > seed
    cat seed | sort | uniq | join - rel.$TOPIC.fil | cut -d' ' -f1 | sed -e 's/^/1 /' > x
    cat seed | sort | uniq | join - rel.$TOPIC.fil -v1 | cut -d' ' -f1 | sed -e 's/^/-1 /' >> x      
    sort -k2 x | join -12 - svm.fil.$TOPIC | cut -d' ' -f2-  | sort -n > trainset
    ../sof*/src/sof*ml --iterations 2000000 --training_file trainset --dimensionality 1100000 --model_out svm_model
      
        
    ../sof*/src/sof*ml --test_file svm.fil.$TOPIC --dimensionality 1100000 --model_in svm_model --results_file pout 
    cat svm.fil.$TOPIC | cut -d' ' -f1 | tr -d ' ' > docfils_temp
 
    python sort.py docfils_temp pout seed 0
    python selectT.py valor_R $int
    export j=`cat flagOut `
    # 
    export t=`wc -l < U$j`

    temp=$(($NDUN-$t))

    tail -$temp newtopic.sort > result

    echo "valor t =$t valor j =$j valor temp =$temp"


export r=`cat result | sort | uniq | join - rel.$TOPIC.fil | uniq | wc -l`

total=`wc -l < rel.$TOPIC.fil`
recall=`echo "scale=5; ($r / $total)" | bc`
precisao=`echo "scale=5; ($r / $temp)" | bc`
echo "resultado final $r ------$temp  recall $recall  precisao $precisao"

